# -*- coding: utf-8 -*-
""" (EX4)model design.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TXXEJwBJkoME3RxNiH4ix-dOM5t8-Seu
"""

# !pip install transformers==4.3.2

import torch
import io
import random
import time
import math
import datetime
import warnings
warnings.filterwarnings('ignore')
import logging
logging.basicConfig(level=logging.ERROR)


from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
from collections import defaultdict

import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,precision_score,recall_score,f1_score
from sklearn.metrics import matthews_corrcoef
# import sentencepiece

import torch.nn.functional as F
import torch.nn as nn
from transformers import *

#!pip install sentencepiece
from sklearn import metrics

import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Metrics_Evaluations/')
import metrics_eval

# address
import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
import Arch

import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Dataloaders/')
import data_loaders
import Data_sampler



# import sys
# sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
# form Fair_NDABERT import


# If there's a GPU available...
if torch.cuda.is_available():
    # Tell PyTorch to use the GPU.
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

#________________________________________________________________________________
epsilon = 1e-8
thres = 0.5
#________________________________________________________________________________



'''
Extra some part
'''



import json
'''
data_dic['dataset_initializer'].get_num_label() = 27
seed_val = 42
data_dic['seed'] = 42
'''
BATCH_SIZE = 32
MAX_LEN = 200


import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
import Arch

import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
from  Parent_Models import TextClassificationModel, DebiasingMethod, TrainingLog, PlottingUtils,Plot_data



import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models/')
from Fair_NDABERT import NDABERT, FairNDABERT
from Fair_GANBERT import GANBERT, FairGANBERT
from Fair_BERT import BERT, FairBERT


#import sys
#sys.path.append('/content/drive/MyDrive/SS_Fair/Experiments/')
#from pt_remover import remove_files_with_pt_format



import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Dataloaders/')
import data_loaders
import Data_sampler

import ast

class wiki_initializer:

  def __init__(self):

    # address
    self.data = pd.read_csv("/content/drive/MyDrive/SS_Fair/Datasets/wiki/train_wiki.csv")
    self.data = self.data.reset_index()
    self.data['attr'] = self.data['attr'].fillna('[]')
    self.data  = self.data.rename(columns={'gender_protected': 'gender', 'race_protected': 'race'})
    #self.data,fair_data_val = train_test_split(self.data, test_size=0.9 , random_state=42)
    #self.data = self.data.reset_index()

    print("train_size", len(self.data))


    self.data_test = pd.read_csv("/content/drive/MyDrive/SS_Fair/Datasets/wiki/test_wiki.csv")
    self.data_test = self.data_test.reset_index()
    self.data_test['attr'] = self.data_test['attr'].fillna('[]')
    self.data_test  = self.data_test.rename(columns={'gender_protected': 'gender', 'race_protected': 'race'})

    print("test_size", len(self.data_test))



    self.percentage = 0




  def sample_unlabeled_data(self,unlabel_non_toxic_dis, unlabel_toxic_dis, label_non_toxic_dis,label_toxic_dis):

    """
    This function samples the unlabeled data for each sub-category (gender, race, and non-protected) based on the desired proportions.

    Parameters:
    - unlabel_non_toxic_dis (list): A list of the desired number of unlabeled non-toxic samples for each sub-category.
    - unlabel_toxic_dis (list): A list of the desired number of unlabeled toxic samples for each sub-category.

    Returns:
    - tuple: A tuple containing the updated `unlabel_non_toxic_dis` and `unlabel_toxic_dis` lists.

    """

    no_toxic_no_protected = len(self.data.query('Label == 0 and `gender` == 0 and `race` == 0'))
    no_toxic_race_protected = len(self.data.query('Label == 0 and `gender` == 0 and `race` == 1'))
    no_toxic_gender_protected = len(self.data.query('Label == 0 and `gender` == 1 and `race` == 0'))
    # no_toxic_both_protected = len(self.data.query('Label == 0 and `gender` == 1 and `race` == 1'))
    toxic_no_protected = len(self.data.query('Label == 1 and `gender` == 0 and `race` == 0'))
    toxic_race_protected = len(self.data.query('Label == 1 and `gender` == 0 and `race` == 1'))
    toxic_gender_protected = len(self.data.query('Label == 1 and `gender` == 1 and `race` == 0'))
    # toxic_both_protected = len(self.data.query('Label == 1 and `gender` == 1 and `race` == 1'))

    num_samples = [no_toxic_no_protected -label_non_toxic_dis[0] , no_toxic_race_protected - label_non_toxic_dis[1], no_toxic_gender_protected -label_non_toxic_dis[2] ,
                   toxic_no_protected-label_toxic_dis[0], toxic_race_protected-label_toxic_dis[1], toxic_gender_protected-label_toxic_dis[2]]

    # for i, (non_toxic_count, toxic_count) in enumerate(zip(unlabel_non_toxic_dis, unlabel_toxic_dis)):

    #     if non_toxic_count > num_samples[i]:
    #         diff = non_toxic_count - num_samples[i]
    #         unlabel_non_toxic_dis[i] -= diff

    #     if toxic_count > num_samples[i + 4]:
    #         diff = toxic_count - num_samples[i + 4]
    #         unlabel_toxic_dis[i] -= diff

    return unlabel_non_toxic_dis, unlabel_toxic_dis




  def sampler_object_experiment1(self,seed = 42, number_unlabel = 5000,percentage = 0.005):

    '''
    -change the code that recieve percentage and calculate the sampling list for hate explain. this one is percentage based, later you can add number based sampler
    generaly we're looking for the distirbution of samples

    -two things are improtant to have the number of fine terms in train\test. also to check how subsampled training set looks like real training data

    - add two-bar for each fine term bar chart based one thier label to make it more informative
    '''


    toxic_label_ratio = [0.1,0.45,0.45]
    non_toxic_label_ratio = [0.8,0.1,0.1]

    #5000
    self.number_unlabel = number_unlabel
    number_label = math.ceil(len(self.data)*percentage)

    self.number_label = number_label
    # self.number_unlabel = len(self.data) - self.number_label


    # print(number_label)
    label_non_toxic_dis = [round(x * number_label) for x in non_toxic_label_ratio]
    # print(label_non_toxic_dis)

    label_toxic_dis = [round(x * number_label) for x in toxic_label_ratio]
    # print(label_toxic_dis)

    no_toxic_no_protected = len(self.data.query('Label == 0 and `gender` == 0 and `race` == 0'))
    no_toxic_race_protected = len(self.data.query('Label == 0 and `gender` == 0 and `race` == 1'))
    no_toxic_gender_protected = len(self.data.query('Label == 0 and `gender` == 1 and `race` == 0'))
    no_toxic_both_protected = len(self.data.query('Label == 0 and `gender` == 1 and `race` == 1'))
    toxic_no_protected = len(self.data.query('Label == 1 and `gender` == 0 and `race` == 0'))
    toxic_race_protected = len(self.data.query('Label == 1 and `gender` == 0 and `race` == 1'))
    toxic_gender_protected = len(self.data.query('Label == 1 and `gender` == 1 and `race` == 0'))
    toxic_both_protected = len(self.data.query('Label == 1 and `gender` == 1 and `race` == 1'))

    # print(number_label)
    # unlabel_non_toxic_dis = [round(x * number_unlabel) for x in non_toxic_label_ratio ]
    # print(label_non_toxic_dis)
    unlabel_non_toxic_dis = np.array([0.1,0.45,0.45])*0.4*number_unlabel
    unlabel_non_toxic_dis = np.rint(unlabel_non_toxic_dis).astype(int)

    # unlabel_toxic_dis = [round(x * number_unlabel) for x in toxic_label_ratio]
    unlabel_toxic_dis = np.array([0.8,0.1,0.1])*0.6*number_unlabel
    unlabel_toxic_dis = np.rint(unlabel_toxic_dis).astype(int)

    # print(label_toxic_dis)


    unlabel_non_toxic_dis, unlabel_toxic_dis = self.sample_unlabeled_data(unlabel_non_toxic_dis, unlabel_toxic_dis, label_non_toxic_dis,label_toxic_dis)

    non_toxic_proportion_list = [label_non_toxic_dis[0],unlabel_non_toxic_dis[0] ,label_non_toxic_dis[1], unlabel_non_toxic_dis[1],label_non_toxic_dis[2], unlabel_non_toxic_dis[2]]
    toxic_proportion_list = [label_toxic_dis[0],unlabel_toxic_dis[0] ,label_toxic_dis[1],unlabel_toxic_dis[1] ,label_toxic_dis[2],unlabel_toxic_dis[2] ]

    if percentage == 1:
      self.percentage  = 1
      non_toxic_proportion_list = [no_toxic_no_protected,0,no_toxic_race_protected,0,no_toxic_gender_protected,0]
      toxic_proportion_list = [toxic_no_protected, 0,toxic_race_protected, 0,toxic_gender_protected,0]


    self.sampler = Data_sampler.SS_Data_Sampler(self.data,non_toxic_proportion_list, toxic_proportion_list,seed)




  def get_sampler(self):

    return self.sampler

  def train_test_sample(self, GANBERT_flag = True, IW_flag = False):

    train= self.sampler.data_sampler(GANBERT_flag,IW_flag)
    test = Data_sampler.SS_Data_Sampler.validator_target( self.data_test, GANBERT_flag, IW_flag)


    return train,test

  def pytorch_loader(self,train, test,batch_size = 16,MAX_LEN = 200):


    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')


    # address
    if self.percentage == 0:
        train_data_loader,debias_dataset = data_loaders.ss_gan_data_loader(train ,tokenizer, MAX_LEN,batch_size )
        test_data_loader,NO_USE = data_loaders.ss_gan_data_loader(test ,tokenizer, MAX_LEN,batch_size)
    else:
      train_data_loader,debias_dataset = data_loaders.create_Fair_data_loader(train ,tokenizer, MAX_LEN,batch_size )
      test_data_loader,NO_USE = data_loaders.ss_gan_data_loader(test ,tokenizer, MAX_LEN,batch_size)




    return train_data_loader,debias_dataset,test_data_loader



  def pop_data_loaders(self,batch_size = 16,MAX_LEN = 200,GANBERT_flag = True, IW_flag = False ):


    train,test = self.train_test_sample(GANBERT_flag,IW_flag)
    # print("HEREEEEEEEEEEEEEEEEEEEEEEEEE", train)
    train_data_loader,debias_dataset,test_data_loader = self.pytorch_loader(train, test,batch_size,MAX_LEN)

    return  train_data_loader,debias_dataset,test_data_loader,train,test


  def get_num_label(self):

    return self.number_label

  def get_num_unlabel(self):
    return self.number_unlabel





import os


class model_initializer:

   def __init__(self):
     pass

   def path_maker(self,folder_path):

      # Split the path into separate directories
      directories = folder_path.split("/")

      # Iterate over the directories and create each one
      path = "/"
      for directory in directories:
          path = os.path.join(path, directory)
          if not os.path.exists(path):
              # Create the directory
              os.makedirs(path)
              print("Directory created: ", path)
          else:
              print("Directory already exists: ", path)



   def Model_initializer_NDABERT_two_NET(self, data_dic, NDA_dic, Fair_NDA_dic,dataset_name = "Noname", Exp = "Exp0"):

      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      # dataset loader
      '''
      recieve the object from outside
      '''

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/"+str(Fair_NDA_dic['lda'])+"/"

      print("1")
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], number_unlabel = data_dic['unlabel_data'] ,percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      '''
      add an address for data here
      '''
      self.path_maker(experiment_path + "/NDABERT/"+ "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "/NDABERT/"+ "seed_"+str(data_dic['seed'])+"/"+ "data/"
      print("2")
      self.path_maker(path_data)

      print("path_data",path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      # architecture initializer
      config = AutoConfig.from_pretrained('distilbert-base-uncased')
      hidden_size = int(config.hidden_size)


      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
      transformer_ndabert_G = DistilBertModel.from_pretrained('distilbert-base-uncased')


      generator_ndabert_G = Arch.Generator(noise_size=NDA_dic['noise_size'], output_size=hidden_size, hidden_sizes=[768], dropout_rate=out_dropout_rate)
      discriminator_ndabert_G = Arch.Discriminator(input_size=hidden_size, hidden_sizes=[768], num_labels=NUM_CLS_G , dropout_rate=out_dropout_rate)

      #seed
      adv_fair_ndabert_GF =Arch.Adversary(data_dic['seed'],2)

      adv_fair_ndabert_GF.to(device)
      generator_ndabert_G.to(device)
      discriminator_ndabert_G.to(device)
      transformer_ndabert_G.to(device)


      # model initializer

      path = experiment_path + "/NDABERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "NDABERT"
      '''
      extra
      '''
      # data_dic['dataset_initializer'].get_num_label() = 20
      # num_unlabel_data = 5000
      # seed = 42

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_NDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_NDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      hyper_param_Pre_training_stage = {
      "transformer":transformer_ndabert_G,
      "discriminator":discriminator_ndabert_G,
      "generator":generator_ndabert_G,
      "adversary":adv_fair_ndabert_GF,
      "supervised_loss": Arch.supervised_loss,
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,


      "seed":NDA_dic['seed'],
      "discriminator_LR":NDA_dic['discriminator_LR'],
      "generator_LR":NDA_dic['generator_LR'],
      "adversary_LR":NDA_dic['adversary_LR'],
      "lda":NDA_dic['lda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      NDABERT_cls = NDABERT(**hyper_param_Pre_training_stage)



      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": NDA_dic['sensitive_term'],
      "epochs_adv":NDA_dic['epochs_adv'],
      "iterations":NDA_dic['iterations'],
      "noise_size":NDA_dic['noise_size'],
      "l":NDA_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = NDABERT_cls.pre_training(**hyper_param_pre_training)




      model_name = "Fair_NDABERT"


      path_fairNda =  experiment_path + "/FairNDABERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairNDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairNDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      hyper_param_Training_eval = {
      "adversary":adv_fair_ndabert_GF,
      "supervised_loss":Arch.supervised_loss,
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_NDA_dic['seed'],
      "discriminator_LR":Fair_NDA_dic['discriminator_LR'],
      "generator_LR":Fair_NDA_dic['generator_LR'],
      "adversary_LR":Fair_NDA_dic['adversary_LR'],
      "lda":Fair_NDA_dic['lda'],
      "batch_size": Fair_NDA_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      "flag_transformer": True
      }

      Adv_debiasing = FairNDABERT(**hyper_param_Training_eval)

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_NDA_dic['sensitive_term'],
      "iterations":Fair_NDA_dic['iterations'],
      "epochs":Fair_NDA_dic['epochs'],
      "num_mini_batch":Fair_NDA_dic['num_mini_batch'],
      "noise_size":Fair_NDA_dic['noise_size'],
      "selection_score":Fair_NDA_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)



      # return [performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine], [performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair],experiment_path


   def Model_initializer_GANBERT_two_NET(self, data_dic, GANBERT_dic, Fair_GANBERT_dic,dataset_name = "Noname", Exp = "Exp0"):

      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/"+str(Fair_GANBERT_dic['lda'])+"/"
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], number_unlabel = data_dic['unlabel_data'] ,percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      self.path_maker(experiment_path + "/GANBERT/"+ "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "/GANBERT/"+ "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # GANBERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      config = AutoConfig.from_pretrained('distilbert-base-uncased')
      hidden_size = int(config.hidden_size)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
      transformer_GANBERT_G = DistilBertModel.from_pretrained('distilbert-base-uncased')


      generator_GANBERT_G = Arch.Generator(noise_size=GANBERT_dic['noise_size'], output_size=hidden_size, hidden_sizes=[768], dropout_rate=out_dropout_rate)
      discriminator_GANBERT_G = Arch.Discriminator(input_size=hidden_size, hidden_sizes=[768], num_labels=NUM_CLS_G , dropout_rate=out_dropout_rate)

      #seed
      adv_fair_GANBERT_GF =Arch.Adversary(data_dic['seed'],2)

      adv_fair_GANBERT_GF.to(device)
      generator_GANBERT_G.to(device)
      discriminator_GANBERT_G.to(device)
      transformer_GANBERT_G.to(device)



      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "/GANBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "GANBERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_GANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_GANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      hyper_param_Pre_training_stage = {
      "transformer":transformer_GANBERT_G,
      "discriminator":discriminator_GANBERT_G,
      "generator":generator_GANBERT_G,
      "adversary":adv_fair_GANBERT_GF,
      "supervised_loss": Arch.supervised_loss,
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":GANBERT_dic['seed'],
      "discriminator_LR":GANBERT_dic['discriminator_LR'],
      "generator_LR":GANBERT_dic['generator_LR'],
      "adversary_LR":GANBERT_dic['adversary_LR'],
      "lda":GANBERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      GANBERT_cls = GANBERT(**hyper_param_Pre_training_stage)



      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": GANBERT_dic['sensitive_term'],
      "epochs_adv":GANBERT_dic['epochs_adv'],
      "iterations":GANBERT_dic['iterations'],
      "noise_size":GANBERT_dic['noise_size'],
      "l":GANBERT_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = GANBERT_cls.pre_training(**hyper_param_pre_training)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # Fair_GANBERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_name = "FairGANBERT"


      path_fairNda =  experiment_path + "/FairGANBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairGANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairGANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      hyper_param_Training_eval = {
      "adversary":adv_fair_GANBERT_GF,
      "supervised_loss":Arch.supervised_loss,
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_GANBERT_dic['seed'],
      "discriminator_LR":Fair_GANBERT_dic['discriminator_LR'],
      "generator_LR":Fair_GANBERT_dic['generator_LR'],
      "adversary_LR":Fair_GANBERT_dic['adversary_LR'],
      "lda":Fair_GANBERT_dic['lda'],
      "batch_size": Fair_GANBERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      "flag_transformer": True
      }

      Adv_debiasing = FairGANBERT(**hyper_param_Training_eval)

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_GANBERT_dic['sensitive_term'],
      "iterations":Fair_GANBERT_dic['iterations'],
      "epochs":Fair_GANBERT_dic['epochs'],
      "num_mini_batch":Fair_GANBERT_dic['num_mini_batch'],
      "noise_size":Fair_GANBERT_dic['noise_size'],
      "selection_score":Fair_GANBERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)



      # return [performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine], [performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair],experiment_path






   def Model_initializer_BERT_NET(self, data_dic, BERT_dic, Fair_BERT_dic,dataset_name = "Noname", Exp = "Exp0"):

      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/"+str(Fair_BERT_dic['lda'])+"/"
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], number_unlabel = data_dic['unlabel_data'] ,percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      self.path_maker(experiment_path + "/BERT/"+ "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "/BERT/"+ "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_FB = Arch.BertClassifier(2)
      model_FB = model_FB.to(device)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

      adv_FB = Arch.Adversary(data_dic['seed'],2)
      adv_FB = adv_FB.to(device)



      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "/BERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "BERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      #(self, classifier, adversary,supervised_loss, loss_adversary, log_training,plot_training,seed,classifier_LR=2e-5, adversary_LR=0.001, lda=[15, 30],batch_size = 32, path="", model_name="", transformer_name=""):

      hyper_param_Pre_training_stage = {
      "classifier":model_FB,
      "adversary":adv_FB,
      "supervised_loss":torch.nn.CrossEntropyLoss(),
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":BERT_dic['seed'],
      "classifier_LR":BERT_dic['classifier_LR'],
      "adversary_LR":BERT_dic['adversary_LR'],
      "lda":BERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      BERT_cls = BERT(**hyper_param_Pre_training_stage)


      #pre_training(self,train_loader,validation_loader,mode= "Validation", epochs_adv = 1, iterations = 1,noise_size = 100, l = 0.85,ippts_loader= None,fine_terms_list = None,path ="", model_name = "",transformer_name = ""):

      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": BERT_dic['sensitive_term'],
      "epochs_adv":BERT_dic['epochs_adv'],
      "iterations":BERT_dic['iterations'],
      "noise_size":BERT_dic['noise_size'],
      "l":BERT_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = BERT_cls.pre_training(**hyper_param_pre_training)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # Fair_BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_name = "FairBERT"


      path_fairNda =  experiment_path + "/FairBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      # (self, adversary,loss_classifier,loss_adversary,optimizer_adv_lr = 0.001,optimizer_cl_lr =1e-5,lda= [1,1],batch_size = 8,path ="", model_name = "",transformer_name = ""):

      hyper_param_Training_eval = {
      "adversary":adv_FB,
      "loss_classifier":torch.nn.CrossEntropyLoss(),
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_BERT_dic['seed'],
      "classifier_LR":BERT_dic['classifier_LR'],
      "adversary_LR":BERT_dic['adversary_LR'],
      "lda":Fair_BERT_dic['lda'],
      "batch_size": Fair_BERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      }

      Adv_debiasing = FairBERT(**hyper_param_Training_eval)

      # train_eval(self, train_loader ,validation_loader,train_set,fairness_metric = "equ_odds_percent",max_shift_acc = 0.07,ippts_loader = None,fine_terms_list = None, iterations = 15,epochs = 1, num_mini_batch =1,noise_size =100, selection_score = "eq_odds",path ="", model_name = "",transformer_name = ""):

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_BERT_dic['sensitive_term'],
      "iterations":Fair_BERT_dic['iterations'],
      "epochs":Fair_BERT_dic['epochs'],
      "num_mini_batch":Fair_BERT_dic['num_mini_batch'],
      "noise_size":Fair_BERT_dic['noise_size'],
      "selection_score":Fair_BERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)



      # return [performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine], [performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair],experiment_path




   def Model_initializer_Full_supervised_BERT_NET(self, data_dic, BERT_dic, Fair_BERT_dic,dataset_name = "Noname", Exp = "Exp0"):

      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/"+str(Fair_BERT_dic['lda'])+"/"
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], number_unlabel = data_dic['unlabel_data'] ,percentage = data_dic['label_ratio'])
      train_data_loader1,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )
      print("train***************************************",train)
      print("1"*30, next(iter(train_data_loader1)))

      self.path_maker(experiment_path + "/FullBERT/"+ "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "/FullBERT/"+ "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_FB = Arch.BertClassifier(2)
      model_FB = model_FB.to(device)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

      adv_FB = Arch.Adversary(data_dic['seed'],2)
      adv_FB = adv_FB.to(device)



      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "/FullBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "FullBERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      #(self, classifier, adversary,supervised_loss, loss_adversary, log_training,plot_training,seed,classifier_LR=2e-5, adversary_LR=0.001, lda=[15, 30],batch_size = 32, path="", model_name="", transformer_name=""):
      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], number_unlabel = data_dic['unlabel_data'] ,percentage = data_dic['label_ratio'])
      train_data_loader1,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      hyper_param_Pre_training_stage = {
      "classifier":model_FB,
      "adversary":adv_FB,
      "supervised_loss":torch.nn.CrossEntropyLoss(),
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":BERT_dic['seed'],
      "classifier_LR":BERT_dic['classifier_LR'],
      "adversary_LR":BERT_dic['adversary_LR'],
      "lda":BERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      BERT_cls = BERT(**hyper_param_Pre_training_stage)


      #pre_training(self,train_loader,validation_loader,mode= "Validation", epochs_adv = 1, iterations = 1,noise_size = 100, l = 0.85,ippts_loader= None,fine_terms_list = None,path ="", model_name = "",transformer_name = ""):
      print("5"*30, next(iter(train_data_loader1)))

      hyper_param_pre_training = {
      "train_loader":train_data_loader1,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": BERT_dic['sensitive_term'],
      "epochs_adv":BERT_dic['epochs_adv'],
      "iterations":BERT_dic['iterations'],
      "noise_size":BERT_dic['noise_size'],
      "l":BERT_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = BERT_cls.pre_training(**hyper_param_pre_training)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # Fair_BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_name = "FairFullBERT"


      path_fairNda =  experiment_path + "/FairFullBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      # (self, adversary,loss_classifier,loss_adversary,optimizer_adv_lr = 0.001,optimizer_cl_lr =1e-5,lda= [1,1],batch_size = 8,path ="", model_name = "",transformer_name = ""):

      hyper_param_Training_eval = {
      "adversary":adv_FB,
      "loss_classifier":torch.nn.CrossEntropyLoss(),
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_BERT_dic['seed'],
      "classifier_LR":BERT_dic['classifier_LR'],
      "adversary_LR":BERT_dic['adversary_LR'],
      "lda":Fair_BERT_dic['lda'],
      "batch_size": Fair_BERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      }

      Adv_debiasing = FairBERT(**hyper_param_Training_eval)

      # train_eval(self, train_loader ,validation_loader,train_set,fairness_metric = "equ_odds_percent",max_shift_acc = 0.07,ippts_loader = None,fine_terms_list = None, iterations = 15,epochs = 1, num_mini_batch =1,noise_size =100, selection_score = "eq_odds",path ="", model_name = "",transformer_name = ""):

      hyper_param_train_eval = {
      "train_loader":train_data_loader1,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_BERT_dic['sensitive_term'],
      "iterations":Fair_BERT_dic['iterations'],
      "epochs":Fair_BERT_dic['epochs'],
      "num_mini_batch":Fair_BERT_dic['num_mini_batch'],
      "noise_size":Fair_BERT_dic['noise_size'],
      "selection_score":Fair_BERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)

'''
you must initialized TrainingLog object here and send it to FairTextClassification and then save the log for each epocha and seed and model
'''
'''
you have one seed for data to set and also for model initalization that you must consider
'''
class FairTextClassification:

    def __init__(self):

        # self.classifier = classifier
        # self.debiasing_method = debiasing_method

        # self.classifier_dic = classifier_dic
        # self.debiasing_dic = debiasing_dic
        # self.data_dic = data_dic

        # self.seed_list = seed_list
        pass



    def Performance_Experience1(self,Model_initializer_list,data_dic, classifier_dic, debiasing_dic, seed_list, label_ratio,dataset_name = "No_name"):




      for i in range(len( classifier_dic)):
        for seed in seed_list:

          print("Seed",seed)

          '''
          - model_initializer class should recieve all the dictuionaries and we changfe the paraeter based on the experience, for example here seeds changes,
            classifier and debiasing_method is the output of model_initializer method

          - then find a way to save them properly.
          - then you have to make plot out of csv saved files and save them.
          - put the codes for all the models in folders.
          - define two experiments.

          '''
          data_dic[i]['label_ratio'] = label_ratio

          print("classifier_dic",classifier_dic)
          print(" Model_initializer_list[i]", Model_initializer_list[i])
          data_dic[i]['seed'] =  seed
          classifier_dic[i]['seed'] =  seed
          debiasing_dic[i]['seed'] =  seed


          Model_initializer_list[i]( data_dic[i], classifier_dic[i], debiasing_dic[i],dataset_name,"Exp4")

##########################################
##########################################
#add different seeds
dataset_name = "wiki"
Exp= "Ex4"


debias_iter = 20
cls_iter = 6
################################################
################################################
class model_initializer:

   def __init__(self):
    pass

   '''
   def path_maker(self, folder_path):

    # Split the path into separate directories
    directories = folder_path.split("/")

    # Iterate over the directories and create each one
    path = "/"
    for directory in directories:
        path = os.path.join(path, directory)
        if not os.path.exists(path):
            # Create the directory
            os.makedirs(path)
            print("Directory created: ", path)
        else:
            # Delete the directory and its contents
            #print("Directory already exists: ", path)
            #shutil.rmtree(path)
            print("Directory deleted: ", path)
            os.makedirs(path)
            print("Directory created: ", path)

   '''

   def path_maker(self,folder_path):

      # Split the path into separate directories
      directories = folder_path.split("/")

      # Iterate over the directories and create each one
      path = "/"
      for directory in directories:
          path = os.path.join(path, directory)
          if not os.path.exists(path):
              # Create the directory
              os.makedirs(path)
              print("Directory created: ", path)
          else:
              print("Directory already exists: ", path)


   def NDABERT_two_NET(self,params):

      # dataset_name = "wiki"

      race_term = set(['african','african american','black','white','european','hispanic','latino','latina','latinx','mexican','canadian','american','asian','indian','middle eastern','chinese','japanese'])


      gender_term = set(['lesbian','gay','bisexual', 'transgender', 'trans','queer','lgbt','lgbtq','homosexual','straight','heterosexual','male','female','nonbinary'])

      sensitive_term = [gender_term,race_term]

      data_dic = {
          'dataset_initializer':wiki_initializer(),
          'seed': 11,
          'label_ratio': 0.0161,
          'batch_size':32,
          'max_len': 200,
          'GANBERT_flag': True,
          'IW_flag':False
          }


      NDA_dic ={
      'label_ratio': 0.0161,
      'seed': 11,

      "lda":[1,1],
      "sensitive_term":sensitive_term,
      "epochs_adv":1,
      # iteration
      "iterations":cls_iter,
      "noise_size":100,
      "Mixup_lambda":0.85,

      }


      '''
      recieve the object from outside
      '''

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/NDABERT/"

      self.path_maker(experiment_path)

      #########################
      # Dataset Loader
      #########################
      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )




      #########################
      # Directory for plot
      #########################
      self.path_maker(experiment_path + "seed_"+str(data_dic['seed']))
      path_data = experiment_path + "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)
      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)
      Plot_data(test ,path_data).plot_generator(seed = data_dic['seed'], dataset = "validation_"+dataset_name)


      #########################
      # Architecture Initializer
      #########################
      config = AutoConfig.from_pretrained('distilbert-base-uncased')
      hidden_size = int(config.hidden_size)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
      transformer_ndabert_G = DistilBertModel.from_pretrained('distilbert-base-uncased')

      out_dropout_rate = 0.3
      NUM_CLS_G = 3
      generator_ndabert_G = Arch.Generator(noise_size=NDA_dic['noise_size'], output_size=hidden_size, hidden_sizes=[768], dropout_rate=out_dropout_rate)
      discriminator_ndabert_G = Arch.Discriminator(input_size=hidden_size, hidden_sizes=[768], num_labels=NUM_CLS_G , dropout_rate=out_dropout_rate)
      adv_fair_ndabert_GF =Arch.Adversary(data_dic['seed'],2)

      adv_fair_ndabert_GF.to(device)
      generator_ndabert_G.to(device)
      discriminator_ndabert_G.to(device)
      transformer_ndabert_G.to(device)


      #########################
      # Model Initializer
      #########################

      path = experiment_path + "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "NDABERT"
      '''
      extra
      '''


      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_NDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_NDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      #########################
      # Dataset Loader
      #########################
      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      hyper_param_Pre_training_stage = {
      "transformer":transformer_ndabert_G,
      "discriminator":discriminator_ndabert_G,
      "generator":generator_ndabert_G,
      "adversary":adv_fair_ndabert_GF,
      "supervised_loss": Arch.supervised_loss,
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,


      "seed":NDA_dic['seed'],
      "discriminator_LR":params['discriminator_LR'],
      "generator_LR":params['generator_LR'],
      "adversary_LR":params['adversary_LR'],
      "lda":NDA_dic['lda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      NDABERT_cls = NDABERT(**hyper_param_Pre_training_stage)



      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": NDA_dic['sensitive_term'],
      "epochs_adv":NDA_dic['epochs_adv'],
      "iterations":NDA_dic['iterations'],
      "noise_size":NDA_dic['noise_size'],
      "l":NDA_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = NDABERT_cls.pre_training(**hyper_param_pre_training)


      return -performance_log['Accuracy']


   def FairNDABERT_two_NET(self, params):


      race_term = set(['african','african american','black','white','european','hispanic','latino','latina','latinx','mexican','canadian','american','asian','indian','middle eastern','chinese','japanese'])


      gender_term = set(['lesbian','gay','bisexual', 'transgender', 'trans','queer','lgbt','lgbtq','homosexual','straight','heterosexual','male','female','nonbinary'])


      sensitive_term = [gender_term,race_term]

      data_dic = {
          'dataset_initializer':wiki_initializer(),
          'seed': 11,
          'label_ratio': 0.0161,
          'batch_size':32,
          'max_len': 200,
          'GANBERT_flag': True,
          'IW_flag':False
      }


      Fair_NDA_dic = {
      'seed': 11,
      "batch_size":32,
      "sensitive_term":sensitive_term,
       "discriminator_LR":5e-5,
      "generator_LR":5e-4,
      'adversary_LR': 0.0001,

      # iteration
      "iterations":debias_iter,
      "epochs":1,
      "num_mini_batch":1,
      "noise_size": 100,
      "selection_score":"eq_odds"
      }

      #########################
      # Dataset Loader
      #########################
      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )




      #########################
      # For Loading the NDABERT ADD BEST MODEL PATH HERE
      #########################

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])
      path = experiment_path + "/NDABERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      path = experiment_path + "/NDABERT/"+ "seed_"+str(data_dic['seed'])+"/"
      path_nda = path
      model_nda = "{}_dis_NDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_NDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])




      model_name = "Fair_NDABERT"

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+ "/" + Exp+"/"

      path_fairNda =  experiment_path + "/FairNDABERT/"+ "seed_"+str(data_dic['seed'])+"/"+str(params['lda_1'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairNDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairNDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      adv_fair_ndabert_GF =Arch.Adversary(data_dic['seed'],2)
      adv_fair_ndabert_GF.to(device)

      hyper_param_Training_eval = {
      "adversary":adv_fair_ndabert_GF,
      "supervised_loss":Arch.supervised_loss,
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_NDA_dic['seed'],
      "discriminator_LR":Fair_NDA_dic['discriminator_LR'],
      "generator_LR":Fair_NDA_dic['generator_LR'],
      "adversary_LR":Fair_NDA_dic['adversary_LR'],
      "lda":[params['lda_1'],params['lda_2']],
      "batch_size": Fair_NDA_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      "flag_transformer": True
      }

      Adv_debiasing = FairNDABERT(**hyper_param_Training_eval)

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_NDA_dic['sensitive_term'],
      "iterations":Fair_NDA_dic['iterations'],
      "epochs":Fair_NDA_dic['epochs'],
      "num_mini_batch":Fair_NDA_dic['num_mini_batch'],
      "noise_size":Fair_NDA_dic['noise_size'],
      "selection_score":Fair_NDA_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)





   def GANBERT_two_NET(self,params):




      race_term = set(['african','african american','black','white','european','hispanic','latino','latina','latinx','mexican','canadian','american','asian','indian','middle eastern','chinese','japanese'])


      gender_term = set(['lesbian','gay','bisexual', 'transgender', 'trans','queer','lgbt','lgbtq','homosexual','straight','heterosexual','male','female','nonbinary'])


      sensitive_term = [gender_term,race_term]

      data_dic = {
          'dataset_initializer':wiki_initializer(),
          'seed': 11,
          'label_ratio': 0.0161,
          'batch_size':32,
          'max_len': 200,
          'GANBERT_flag': True,
          'IW_flag':False
      }


      GANBERT_dic ={
      'seed': 11,
      "lda":[1,1],
      "sensitive_term":sensitive_term,

      "discriminator_LR":5e-5,
      "generator_LR":5e-4,
      'adversary_LR': 0.0001,
      "epochs_adv":1,
      # iteration

      "iterations":cls_iter,
      "noise_size":100,
      "Mixup_lambda":0.85,
      }




      out_dropout_rate = 0.3
      NUM_CLS_G = 3


      #experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +"/GANBERT/"+str(data_dic['label_ratio'])+"/"+ str(params['discriminator_LR'])+str(params['adversary_LR'])+str(params['generator_LR'])+"/"


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/GANBERT/"

      self.path_maker(experiment_path)

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------

      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      self.path_maker(experiment_path + "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # GANBERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      config = AutoConfig.from_pretrained('distilbert-base-uncased')
      hidden_size = int(config.hidden_size)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
      transformer_GANBERT_G = DistilBertModel.from_pretrained('distilbert-base-uncased')


      generator_GANBERT_G = Arch.Generator(noise_size=GANBERT_dic['noise_size'], output_size=hidden_size, hidden_sizes=[768], dropout_rate=out_dropout_rate)
      discriminator_GANBERT_G = Arch.Discriminator(input_size=hidden_size, hidden_sizes=[768], num_labels=NUM_CLS_G , dropout_rate=out_dropout_rate)

      #seed
      adv_fair_GANBERT_GF =Arch.Adversary(data_dic['seed'],2)

      adv_fair_GANBERT_GF.to(device)
      generator_GANBERT_G.to(device)
      discriminator_GANBERT_G.to(device)
      transformer_GANBERT_G.to(device)



      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "GANBERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_GANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_GANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      hyper_param_Pre_training_stage = {
      "transformer":transformer_GANBERT_G,
      "discriminator":discriminator_GANBERT_G,
      "generator":generator_GANBERT_G,
      "adversary":adv_fair_GANBERT_GF,
      "supervised_loss": Arch.supervised_loss,
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":GANBERT_dic['seed'],
      "discriminator_LR":params['discriminator_LR'],
      "generator_LR":params['generator_LR'],
      "adversary_LR":params['adversary_LR'],
      "lda":GANBERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      GANBERT_cls = GANBERT(**hyper_param_Pre_training_stage)



      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": GANBERT_dic['sensitive_term'],
      "epochs_adv":GANBERT_dic['epochs_adv'],
      "iterations":GANBERT_dic['iterations'],
      "noise_size":GANBERT_dic['noise_size'],
      "l":GANBERT_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = GANBERT_cls.pre_training(**hyper_param_pre_training)
      return  -performance_log['Accuracy']




   #-----------------------------------------------------------------------------------------------------------------
   #-----------------------------------------------------------------------------------------------------------------
   # Fair_GANBERT
   #-----------------------------------------------------------------------------------------------------------------
   #-----------------------------------------------------------------------------------------------------------------

   def FairGANBERT_two_NET(self,params):





      race_term = set(['african','african american','black','white','european','hispanic','latino','latina','latinx','mexican','canadian','american','asian','indian','middle eastern','chinese','japanese'])


      gender_term = set(['lesbian','gay','bisexual', 'transgender', 'trans','queer','lgbt','lgbtq','homosexual','straight','heterosexual','male','female','nonbinary'])


      sensitive_term = [gender_term,race_term]

      data_dic = {
          'dataset_initializer':wiki_initializer(),
          'seed': 11,
          'label_ratio': 0.0161,
          'batch_size':32,
          'max_len': 200,
          'GANBERT_flag': True,
          'IW_flag':False
      }

      Fair_GANBERT_dic = {
      'seed': 11,
      "batch_size":32,
      "sensitive_term":sensitive_term,
      "discriminator_LR":5e-5,
      "generator_LR":5e-4,
      'adversary_LR': 0.0001,
      # iteration

      "iterations":debias_iter,
      "epochs":1,
      "num_mini_batch":1,
      "noise_size": 100,
      "selection_score":"eq_odds"
      }



      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------

      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      #############################
      #############################
      ### ADD the Best model address
      #############################
      #############################

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/"
      path_nda = experiment_path + "/GANBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      model_nda = "{}_dis_GANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_GANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])




      model_name = "FairGANBERT"

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +"/FairGANBERT/"+str(data_dic['label_ratio'])+"/"

      path_fairNda =  experiment_path + "/FairGANBERT/"+ "seed_"+str(data_dic['seed'])+"/"+str(params['lda_1'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairGANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairGANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      adv_fair_GANBERT_GF =Arch.Adversary(data_dic['seed'],2)
      adv_fair_GANBERT_GF.to(device)

      hyper_param_Training_eval = {
      "adversary":adv_fair_GANBERT_GF,
      "supervised_loss":Arch.supervised_loss,
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_GANBERT_dic['seed'],
      "discriminator_LR":Fair_GANBERT_dic['discriminator_LR'],
      "generator_LR":3e-4,
      "adversary_LR":Fair_GANBERT_dic['adversary_LR'],
      "lda":[params['lda_1'],params['lda_2']],
      "batch_size": Fair_GANBERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      "flag_transformer": True
      }

      Adv_debiasing = FairGANBERT(**hyper_param_Training_eval)

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_GANBERT_dic['sensitive_term'],
      "iterations":Fair_GANBERT_dic['iterations'],
      "epochs":Fair_GANBERT_dic['epochs'],
      "num_mini_batch":Fair_GANBERT_dic['num_mini_batch'],
      "noise_size":Fair_GANBERT_dic['noise_size'],
      "selection_score":Fair_GANBERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)







   def BERT_NET(self,params):


      race_term = set(['african','african american','black','white','european','hispanic','latino','latina','latinx','mexican','canadian','american','asian','indian','middle eastern','chinese','japanese'])


      gender_term = set(['lesbian','gay','bisexual', 'transgender', 'trans','queer','lgbt','lgbtq','homosexual','straight','heterosexual','male','female','nonbinary'])


      sensitive_term = [gender_term,race_term]




      data_dic = {
        'dataset_initializer':wiki_initializer(),
        'seed': 11,
        'label_ratio': 0.0161,
        'batch_size':32,
        'max_len': 200,
        'GANBERT_flag': False,
        'IW_flag':False
        }


      BERT_dic={
      "seed":11,
      "lda":[1,1],
      "sensitive_term": sensitive_term,
      "classifier_LR":2e-5,
      'adversary_LR': 0.0001,
      "epochs_adv":1,
      # iteration
      "iterations":cls_iter,
      "noise_size":100,
      "Mixup_lambda":None,
      }



      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------


      #experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +"/BERT/"+str(data_dic['label_ratio'])+"/"+ str(params['classifier_LR'])+str(params['adversary_LR'])+"/"

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/BERT/"

      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      self.path_maker(experiment_path + "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_FB = Arch.BertClassifier(2)
      model_FB = model_FB.to(device)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

      adv_FB = Arch.Adversary(data_dic['seed'],2)
      adv_FB = adv_FB.to(device)



      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "BERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])


      hyper_param_Pre_training_stage = {
      "classifier":model_FB,
      "adversary":adv_FB,
      "supervised_loss":torch.nn.CrossEntropyLoss(),
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":BERT_dic['seed'],
      "classifier_LR":params['classifier_LR'],
      "adversary_LR":params['adversary_LR'],
      "lda":BERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      BERT_cls = BERT(**hyper_param_Pre_training_stage)



      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": BERT_dic['sensitive_term'],
      "epochs_adv":BERT_dic['epochs_adv'],
      "iterations":BERT_dic['iterations'],
      "noise_size":BERT_dic['noise_size'],
      "l":BERT_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = BERT_cls.pre_training(**hyper_param_pre_training)






   def FairBERT_NET(self,params):


      race_term = set(['african','african american','black','white','european','hispanic','latino','latina','latinx','mexican','canadian','american','asian','indian','middle eastern','chinese','japanese'])


      gender_term = set(['lesbian','gay','bisexual', 'transgender', 'trans','queer','lgbt','lgbtq','homosexual','straight','heterosexual','male','female','nonbinary'])


      sensitive_term = [gender_term,race_term]




      data_dic = {
        'dataset_initializer':wiki_initializer(),
        'seed': 11,
        'label_ratio': 0.0161,
        'batch_size':32,
        'max_len': 200,
        'GANBERT_flag': False,
        'IW_flag':False
        }


      Fair_BERT_dic= {
      "seed":11,
      "batch_size":32,
      "sensitive_term":sensitive_term,
      "classifier_LR":2e-5,
      'adversary_LR': 0.0001,
      # iteration
      "iterations":debias_iter,
      "Mixup_lambda":0.85,
      "epochs":1,
      "num_mini_batch":1,
      "noise_size": 100,
      "selection_score":"eq_odds"
      }




      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +str(data_dic['label_ratio'])+"/BERT/"

      self.path_maker(experiment_path)

      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      #############################
      #############################
      ### ADD the Best model address
      #############################
      #############################

      path_nda = experiment_path + "seed_"+str(data_dic['seed'])+"/"
      model_nda = "{}_dis_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # Fair_BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_name = "FairBERT"


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +"FairBERT/"

      self.path_maker(experiment_path)


      path_fairNda =  experiment_path + "seed_"+str(data_dic['seed'])+"/"+str(params['lda_1'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )



      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      # (self, adversary,loss_classifier,loss_adversary,optimizer_adv_lr = 0.001,optimizer_cl_lr =1e-5,lda= [1,1],batch_size = 8,path ="", model_name = "",transformer_name = ""):
      adv_FB = Arch.Adversary(data_dic['seed'],2)
      adv_FB = adv_FB.to(device)

      hyper_param_Training_eval = {
      "adversary":adv_FB,
      "loss_classifier":torch.nn.CrossEntropyLoss(),
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_BERT_dic['seed'],
      "classifier_LR":Fair_BERT_dic['classifier_LR'],
      "adversary_LR":Fair_BERT_dic['adversary_LR'],
      "lda":[params['lda_1'],params['lda_2']],
      "batch_size": Fair_BERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      }

      Adv_debiasing = FairBERT(**hyper_param_Training_eval)

      # train_eval(self, train_loader ,validation_loader,train_set,fairness_metric = "equ_odds_percent",max_shift_acc = 0.07,ippts_loader = None,fine_terms_list = None, iterations = 15,epochs = 1, num_mini_batch =1,noise_size =100, selection_score = "eq_odds",path ="", model_name = "",transformer_name = ""):

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_BERT_dic['sensitive_term'],
      "iterations":Fair_BERT_dic['iterations'],
      "epochs":Fair_BERT_dic['epochs'],
      "num_mini_batch":Fair_BERT_dic['num_mini_batch'],
      "noise_size":Fair_BERT_dic['noise_size'],
      "selection_score":Fair_BERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)






   def FullBERT_NET(self,params):



      race_term = set(['african','african american','black','white','european','hispanic','latino','latina','latinx','mexican','canadian','american','asian','indian','middle eastern','chinese','japanese'])


      gender_term = set(['lesbian','gay','bisexual', 'transgender', 'trans','queer','lgbt','lgbtq','homosexual','straight','heterosexual','male','female','nonbinary'])


      sensitive_term = [gender_term,race_term]




      data_dic = {
        'dataset_initializer':wiki_initializer(),
        'seed': 11,
        'label_ratio': 0.0161,
        'batch_size':32,
        'max_len': 200,
        'GANBERT_flag': False,
        'IW_flag':False
        }


      BERT_dic={
      "seed":11,
      "lda":[1,1],
      "sensitive_term": sensitive_term,
      "classifier_LR":2e-5,
      'adversary_LR': 0.0001,
      "epochs_adv":1,
      # iteration

      "iterations":cls_iter,
      "noise_size":100,
      "Mixup_lambda":None,
      }



      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      print("111111111111111111111111111111")
      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = 1)
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      #experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +"/FullBERT/"+str(data_dic['label_ratio'])+"/"+ str(params['classifier_LR'])+str(params['adversary_LR'])+"/"

      #experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +"/FullBERT/"+str(data_dic['label_ratio'])+"/"

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/" + Exp+"/" +dataset_name+"/FullBERT/"
      self.path_maker(experiment_path)


      self.path_maker(experiment_path + "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------

      print("2222222222222222222222222222222222222222222222")

      model_FB = Arch.BertClassifier(2)
      model_FB = model_FB.to(device)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

      adv_FB = Arch.Adversary(data_dic['seed'],2)
      adv_FB = adv_FB.to(device)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "FullBERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      print("333333333333333333333333333333333333333333333333333333333333333")
      print(next(iter(train_data_loader)))
      print("44444444444444444444444444444444444444444444444444444444444")

      hyper_param_Pre_training_stage = {
      "classifier":model_FB,
      "adversary":adv_FB,
      "supervised_loss":torch.nn.CrossEntropyLoss(),
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":BERT_dic['seed'],
      "classifier_LR":params['classifier_LR'],
      "adversary_LR":params['adversary_LR'],
      "lda":BERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      print("555555555555555555555555555555555555555555555555555555555555555555555")

      BERT_cls = BERT(**hyper_param_Pre_training_stage)
      print("666666666666666666666666666666666666666666666666666666666666666666666")


      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": BERT_dic['sensitive_term'],
      "epochs_adv":BERT_dic['epochs_adv'],
      "iterations":BERT_dic['iterations'],
      "noise_size":BERT_dic['noise_size'],
      "l":BERT_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }
      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = BERT_cls.pre_training(**hyper_param_pre_training)





   def FullFairBERT_NET(self,params):



      race_term = set(['african','african american','black','white','european','hispanic','latino','latina','latinx','mexican','canadian','american','asian','indian','middle eastern','chinese','japanese'])


      gender_term = set(['lesbian','gay','bisexual', 'transgender', 'trans','queer','lgbt','lgbtq','homosexual','straight','heterosexual','male','female','nonbinary'])


      sensitive_term = [gender_term,race_term]



      data_dic = {
        'dataset_initializer':wiki_initializer(),
        'seed': 11,
        'label_ratio': 0.0161,
        'batch_size':32,
        'max_len': 200,
        'GANBERT_flag': False,
        'IW_flag':False
        }


      Fair_BERT_dic= {
      "seed":11,
      "batch_size":32,
      "sensitive_term":sensitive_term,
      "classifier_LR":2e-5,
      'adversary_LR': 0.0001,
      # iteration

      "iterations":debias_iter,
      "Mixup_lambda":0.85,
      "epochs":1,
      "num_mini_batch":1,
      "noise_size": 100,
      "selection_score":"eq_odds"
      }




      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------

      #############################
      #############################
      ### ADD the Best model address
      #############################
      #############################
      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/" + Exp+"/" +dataset_name+"/FullBERT/"
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )




      path_nda = experiment_path +"seed_"+str(data_dic['seed'])+"/"
      model_nda = "{}_dis_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # Fair_BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_name = "FairFullBERT"

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/"+dataset_name+"/" +Exp+"/" +"/FairFullBERT/"+str(data_dic['label_ratio'])+"/"
      path_fairNda =  experiment_path + "/FairFullBERT/"+ "seed_"+str(data_dic['seed'])+"/"+str(params['lda_1'])+"/"
      self.path_maker(path_fairNda)
      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      model_fairNda = "{}_dis_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])




      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      adv_FB = Arch.Adversary(data_dic['seed'],2)
      adv_FB = adv_FB.to(device)

      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )
      hyper_param_Training_eval = {
      "adversary":adv_FB,
      "loss_classifier":torch.nn.CrossEntropyLoss(),
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_BERT_dic['seed'],
      "classifier_LR":Fair_BERT_dic['classifier_LR'],
      "adversary_LR":Fair_BERT_dic['adversary_LR'],
      "lda": [params['lda_1'],params['lda_2']],
      "batch_size": Fair_BERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      }

      Adv_debiasing = FairBERT(**hyper_param_Training_eval)


      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_BERT_dic['sensitive_term'],
      "iterations":Fair_BERT_dic['iterations'],
      "epochs":Fair_BERT_dic['epochs'],
      "num_mini_batch":Fair_BERT_dic['num_mini_batch'],
      "noise_size":Fair_BERT_dic['noise_size'],
      "selection_score":Fair_BERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)



'''
you must initialized TrainingLog object here and send it to FairTextClassification and then save the log for each epocha and seed and model
'''
'''
you have one seed for data to set and also for model initalization that you must consider
'''

class FairTextClassification:

    def __init__(self,dataset_name = "wiki", label_ratio = 0.05):

        self.Exp= "EXP4"
        self.experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/" + self.Exp+"/" +dataset_name+"/"+str(label_ratio)

        self.label_ratio = label_ratio


    def path_maker(self,folder_path):

      # Split the path into separate directories
      directories = folder_path.split("/")

      # Iterate over the directories and create each one
      path = "/"
      for directory in directories:
          path = os.path.join(path, directory)
          if not os.path.exists(path):
              # Create the directory
              os.makedirs(path)
              print("Directory created: ", path)
          else:
              print("Directory already exists: ", path)



    def HPT_NDABERT(self,dataset_name = "wiki"):

          """
          save the best model
          """


          #   best_model = {'discriminator_LR': 4e-5,
          #  'generator_LR': 7e-4,
          #  'adversary_LR': 0.0001}

          best_model = {'discriminator_LR': 5e-5,
          'generator_LR': 3e-4,
          'adversary_LR': 0.0001}


          model_initializer().NDABERT_two_NET(best_model)





    def HPT_FairNDABERT(self,lda,dataset_name = "wiki"):




        space = {

         'lda_1':  lda,
         'lda_2':  lda}


        model_initializer().FairNDABERT_two_NET(space)


    def HPT_GANBERT(self, dataset_name = "wiki"):

          """
          save the best model
          """


          best_model = {'discriminator_LR': 5e-5,
         'generator_LR': 5e-4,
         'adversary_LR': 0.0001}

          model_initializer().GANBERT_two_NET(best_model)



    def HPT_FairGANBERT(self,lda,dataset_name = "wiki"):




       space = {

         'lda_1':  lda,
         'lda_2':  lda}



       model_initializer().FairGANBERT_two_NET(space)


    def HPT_BERT(self,dataset_name = "wiki"):

          """
          save the best model
          """


          best_model = {'classifier_LR': 2e-5,
                  'adversary_LR': 0.0001}



          model_initializer().BERT_NET(best_model)



    def HPT_FairBERT(self,lda,dataset_name = "wiki"):




         space = {

         'lda_1':  lda,
         'lda_2':  lda}


         model_initializer().FairBERT_NET(space)



    def HPT_FullBERT(self,dataset_name = "wiki"):

          """
          save the best model
          """


          best_model = {'classifier_LR': 2e-5,
                  'adversary_LR': 0.0001}



          model_initializer().FullBERT_NET(best_model)



    def HPT_FullFairBERT(self,lda,dataset_name = "wiki"):




         space = {

         'lda_1':  lda,
         'lda_2':  lda}

         model_initializer().FullFairBERT_NET(space)






    def Ex4(self, dataset_name = "wiki"):

      print("_"*20)

      print("_"*20)
      lda_list = [0.8,1,5,10,20,50,100]



      print("NDABERT")

      '''
      self.HPT_NDABERT(dataset_name)

      for lda in lda_list:
        best_FairNdaBert = self.HPT_FairNDABERT(lda, dataset_name)

      root_path = "/content/drive/MyDrive/SS_Fair/Experiments/wiki/Ex4"
      #remove_files_with_pt_format(root_path)
      '''
      print("GANBERT")

      self.HPT_GANBERT(dataset_name)
      for lda in lda_list:
        best_FairGANBERT = self.HPT_FairGANBERT(lda, dataset_name)

      root_path = "/content/drive/MyDrive/SS_Fair/Experiments/wiki/Ex4"
      #remove_files_with_pt_format(root_path)



      print("BERT")
      self.HPT_BERT(dataset_name)
      for lda in lda_list:
        best_FairBERT = self.HPT_FairBERT(lda, dataset_name)

      root_path = "/content/drive/MyDrive/SS_Fair/Experiments/wiki/Ex4"
      #remove_files_with_pt_format(root_path)


      self.HPT_FullBERT(dataset_name)
      for lda in lda_list:
        best_FairGANBERT = self.HPT_FullFairBERT(lda, dataset_name)

      root_path = "/content/drive/MyDrive/SS_Fair/Experiments/wiki/Ex4"
      #remove_files_with_pt_format(root_path)




FairTextClassification().Ex4("wiki")
