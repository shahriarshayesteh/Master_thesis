# -*- coding: utf-8 -*-
"""hyper_param_model design.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IaHBd4wGQB8AFhMv9Cmwv2ax47WTPU9E
"""

import torch
import io
import random
import time
import math
import datetime
import warnings
warnings.filterwarnings('ignore')
import logging
logging.basicConfig(level=logging.ERROR)
import ast


from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
from collections import defaultdict

import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,precision_score,recall_score,f1_score
from sklearn.metrics import matthews_corrcoef
# import sentencepiece

import torch.nn.functional as F
import torch.nn as nn
from transformers import *

#!pip install sentencepiece
from sklearn import metrics

import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Metrics_Evaluations/')
import metrics_eval

# address
import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
import Arch

import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Dataloaders/')
import data_loaders
import Data_sampler


import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Experiments/')
from pt_remover import remove_files_with_pt_format

# import sys
# sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
# form Fair_NDABERT import


# If there's a GPU available...
if torch.cuda.is_available():
    # Tell PyTorch to use the GPU.
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

#________________________________________________________________________________
epsilon = 1e-8
thres = 0.5
#________________________________________________________________________________

import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
import Arch

import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
from  Parent_Models import TextClassificationModel, DebiasingMethod, TrainingLog, PlottingUtils


import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models/')
from Fair_NDABERT import NDABERT, FairNDABERT
from Fair_GANBERT import GANBERT, FairGANBERT
from Fair_BERT import BERT, FairBERT

import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Models')
from  Parent_Models import TextClassificationModel, DebiasingMethod, TrainingLog, PlottingUtils,Plot_data


import os

from hyperopt import fmin, tpe, hp



import sys
sys.path.append('/content/drive/MyDrive/SS_Fair/Dataloaders/')
import data_loaders
import Data_sampler

import ast




class jigsaw_initializer:

  def __init__(self):

    # address
    self.data = pd.read_csv("/content/drive/MyDrive/SS_Fair/Datasets/Jigsaw/jigsaw_train_small.csv")
    self.data = self.data.reset_index()
    self.data['attr'] = self.data['attr'].fillna('[]')
    self.data  = self.data.rename(columns={'gender_protected': 'gender', 'race_protected': 'race'})
    #self.data,fair_data_val = train_test_split(self.data, test_size=0.9 , random_state=42)
    #self.data = self.data.reset_index()
    
    print("train_size", len(self.data))
  

    self.data_test = pd.read_csv("/content/drive/MyDrive/SS_Fair/Datasets/Jigsaw/jigsaw_test_small.csv")
    self.data_test = self.data_test.reset_index()
    self.data_test['attr'] = self.data_test['attr'].fillna('[]')
    self.data_test  = self.data_test.rename(columns={'gender_protected': 'gender', 'race_protected': 'race'})
    #self.data_test,fair_data_val = train_test_split(self.data_test, test_size=0.69 , random_state=42)
    #self.data_test = self.data_test.reset_index()

    
    print("test_size", len(self.data_test))

   

    self.percentage = 0
   





  def sampler_object_experiment1(self,seed = 42, percentage = 0.005):

    '''
    -change the code that recieve percentage and calculate the sampling list for hate explain. this one is percentage based, later you can add number based sampler
    generaly we're looking for the distirbution of samples

    -two things are improtant to have the number of fine terms in train\test. also to check how subsampled training set looks like real training data

    - add two-bar for each fine term bar chart based one thier label to make it more informative
    '''


    toxic_label_ratio = [0.1,0.45,0.45]
    non_toxic_label_ratio = [0.8,0.1,0.1]


    number_label = math.ceil(len(self.data)*percentage)

    self.number_label = number_label
    self.number_unlabel = len(self.data) - self.number_label


    # print(number_label)
    label_non_toxic_dis = [round(x * number_label) for x in non_toxic_label_ratio]
    # print(label_non_toxic_dis)

    label_toxic_dis = [round(x * number_label) for x in toxic_label_ratio]
    # print(label_toxic_dis)

    no_toxic_no_protected = len(self.data.query('Label == 0 and `gender` == 0 and `race` == 0'))
    no_toxic_race_protected = len(self.data.query('Label == 0 and `gender` == 0 and `race` == 1'))
    no_toxic_gender_protected = len(self.data.query('Label == 0 and `gender` == 1 and `race` == 0'))
    no_toxic_both_protected = len(self.data.query('Label == 0 and `gender` == 1 and `race` == 1'))
    toxic_no_protected = len(self.data.query('Label == 1 and `gender` == 0 and `race` == 0'))
    toxic_race_protected = len(self.data.query('Label == 1 and `gender` == 0 and `race` == 1'))
    toxic_gender_protected = len(self.data.query('Label == 1 and `gender` == 1 and `race` == 0'))
    toxic_both_protected = len(self.data.query('Label == 1 and `gender` == 1 and `race` == 1'))



    non_toxic_proportion_list = [label_non_toxic_dis[0],no_toxic_no_protected - label_non_toxic_dis[0],label_non_toxic_dis[1],no_toxic_race_protected-label_non_toxic_dis[1],label_non_toxic_dis[2],no_toxic_gender_protected -label_non_toxic_dis[2]]
    toxic_proportion_list = [label_toxic_dis[0],toxic_no_protected -label_toxic_dis[0],label_toxic_dis[1],toxic_race_protected -label_toxic_dis[1],label_toxic_dis[2],toxic_gender_protected-label_toxic_dis[2]]

    if percentage == 1:
      self.percentage  = 1
      non_toxic_proportion_list = [no_toxic_no_protected,0,no_toxic_race_protected,0,no_toxic_gender_protected,0]
      toxic_proportion_list = [toxic_no_protected, 0,toxic_race_protected, 0,toxic_gender_protected,0]



    self.sampler = Data_sampler.SS_Data_Sampler(self.data,non_toxic_proportion_list, toxic_proportion_list,seed)



  def get_sampler(self):

    return self.sampler

  def train_test_sample(self, GANBERT_flag = True, IW_flag = False):

    train= self.sampler.data_sampler(GANBERT_flag,IW_flag)
    test = Data_sampler.SS_Data_Sampler.validator_target( self.data_test, GANBERT_flag, IW_flag)


    return train,test

  def pytorch_loader(self,train, test,batch_size = 16,MAX_LEN = 200):


    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')


    # address
    if self.percentage == 0:
        train_data_loader,debias_dataset = data_loaders.ss_gan_data_loader(train ,tokenizer, MAX_LEN,batch_size )
        test_data_loader,NO_USE = data_loaders.ss_gan_data_loader(test ,tokenizer, MAX_LEN,batch_size)
    else:
      train_data_loader,debias_dataset = data_loaders.create_Fair_data_loader(train ,tokenizer, MAX_LEN,batch_size )
      test_data_loader,NO_USE = data_loaders.ss_gan_data_loader(test ,tokenizer, MAX_LEN,batch_size)




    return train_data_loader,debias_dataset,test_data_loader



  def pop_data_loaders(self,batch_size = 16,MAX_LEN = 200,GANBERT_flag = True, IW_flag = False ):


    train,test = self.train_test_sample(GANBERT_flag,IW_flag)
    # print("HEREEEEEEEEEEEEEEEEEEEEEEEEE", train)
    train_data_loader,debias_dataset,test_data_loader = self.pytorch_loader(train, test,batch_size,MAX_LEN)

    return  train_data_loader,debias_dataset,test_data_loader,train,test


  def get_num_label(self):

    return self.number_label

  def get_num_unlabel(self):
    return self.number_unlabel












class model_initializer:

   def __init__(self):
    pass


   def path_maker(self,folder_path):

      # Split the path into separate directories
      directories = folder_path.split("/")

      # Iterate over the directories and create each one
      path = "/"
      for directory in directories:
          path = os.path.join(path, directory)
          if not os.path.exists(path):
              # Create the directory
              os.makedirs(path)
              print("Directory created: ", path)
          else:
              print("Directory already exists: ", path)


   def Model_initializer_NDABERT_two_NET(self, data_dic, NDA_dic, Fair_NDA_dic,dataset_name = "Noname", Exp = "Exp0"):

      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      # dataset loader
      '''
      recieve the object from outside
      '''

      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/" +dataset_name+"/"+ Exp+"/"+str(data_dic['label_ratio'])+"/"

      print("1")
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      '''
      add an address for data here
      '''
      self.path_maker(experiment_path + "/NDABERT/"+ "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "/NDABERT/"+ "seed_"+str(data_dic['seed'])+"/"+ "data/"
      print("2")
      self.path_maker(path_data)

      print("path_data",path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      # architecture initializer
      config = AutoConfig.from_pretrained('distilbert-base-uncased')
      hidden_size = int(config.hidden_size)


      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
      transformer_ndabert_G = DistilBertModel.from_pretrained('distilbert-base-uncased')


      generator_ndabert_G = Arch.Generator(noise_size=NDA_dic['noise_size'], output_size=hidden_size, hidden_sizes=[768], dropout_rate=out_dropout_rate)
      discriminator_ndabert_G = Arch.Discriminator(input_size=hidden_size, hidden_sizes=[768], num_labels=NUM_CLS_G , dropout_rate=out_dropout_rate)

      #seed
      adv_fair_ndabert_GF =Arch.Adversary(data_dic['seed'],2)

      adv_fair_ndabert_GF.to(device)
      generator_ndabert_G.to(device)
      discriminator_ndabert_G.to(device)
      transformer_ndabert_G.to(device)


      # model initializer

      path = experiment_path + "/NDABERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "NDABERT"
      '''
      extra
      '''
      # num_label_data = 20
      # num_unlabel_data = 5000
      # seed = 42

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_NDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_NDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      hyper_param_Pre_training_stage = {
      "transformer":transformer_ndabert_G,
      "discriminator":discriminator_ndabert_G,
      "generator":generator_ndabert_G,
      "adversary":adv_fair_ndabert_GF,
      "supervised_loss": Arch.supervised_loss,
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,


      "seed":NDA_dic['seed'],
      "discriminator_LR":NDA_dic['discriminator_LR'],
      "generator_LR":NDA_dic['generator_LR'],
      "adversary_LR":NDA_dic['adversary_LR'],
      "lda":NDA_dic['lda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      NDABERT_cls = NDABERT(**hyper_param_Pre_training_stage)



      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": NDA_dic['sensitive_term'],
      "epochs_adv":NDA_dic['epochs_adv'],
      "iterations":NDA_dic['iterations'],
      "noise_size":NDA_dic['noise_size'],
      "l":NDA_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = NDABERT_cls.pre_training(**hyper_param_pre_training)




      model_name = "Fair_NDABERT"


      path_fairNda =  experiment_path + "/FairNDABERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairNDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairNDABERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      hyper_param_Training_eval = {
      "adversary":adv_fair_ndabert_GF,
      "supervised_loss":Arch.supervised_loss,
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_NDA_dic['seed'],
      "discriminator_LR":Fair_NDA_dic['discriminator_LR'],
      "generator_LR":Fair_NDA_dic['generator_LR'],
      "adversary_LR":Fair_NDA_dic['adversary_LR'],
      "lda":Fair_NDA_dic['lda'],
      "batch_size": Fair_NDA_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      "flag_transformer": True
      }

      Adv_debiasing = FairNDABERT(**hyper_param_Training_eval)

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_NDA_dic['sensitive_term'],
      "iterations":Fair_NDA_dic['iterations'],
      "epochs":Fair_NDA_dic['epochs'],
      "num_mini_batch":Fair_NDA_dic['num_mini_batch'],
      "noise_size":Fair_NDA_dic['noise_size'],
      "selection_score":Fair_NDA_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)



      # return [performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine], [performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair],experiment_path


   def Model_initializer_GANBERT_two_NET(self, data_dic, GANBERT_dic, Fair_GANBERT_dic,dataset_name = "Noname", Exp = "Exp0"):

      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/" +dataset_name+"/"+ Exp+"/"+str(data_dic['label_ratio'])+"/"
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      self.path_maker(experiment_path + "/GANBERT/"+ "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "/GANBERT/"+ "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # GANBERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      config = AutoConfig.from_pretrained('distilbert-base-uncased')
      hidden_size = int(config.hidden_size)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
      transformer_GANBERT_G = DistilBertModel.from_pretrained('distilbert-base-uncased')


      generator_GANBERT_G = Arch.Generator(noise_size=GANBERT_dic['noise_size'], output_size=hidden_size, hidden_sizes=[768], dropout_rate=out_dropout_rate)
      discriminator_GANBERT_G = Arch.Discriminator(input_size=hidden_size, hidden_sizes=[768], num_labels=NUM_CLS_G , dropout_rate=out_dropout_rate)

      #seed
      adv_fair_GANBERT_GF =Arch.Adversary(data_dic['seed'],2)

      adv_fair_GANBERT_GF.to(device)
      generator_GANBERT_G.to(device)
      discriminator_GANBERT_G.to(device)
      transformer_GANBERT_G.to(device)



      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "/GANBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "GANBERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_GANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_GANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      hyper_param_Pre_training_stage = {
      "transformer":transformer_GANBERT_G,
      "discriminator":discriminator_GANBERT_G,
      "generator":generator_GANBERT_G,
      "adversary":adv_fair_GANBERT_GF,
      "supervised_loss": Arch.supervised_loss,
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":GANBERT_dic['seed'],
      "discriminator_LR":GANBERT_dic['discriminator_LR'],
      "generator_LR":GANBERT_dic['generator_LR'],
      "adversary_LR":GANBERT_dic['adversary_LR'],
      "lda":GANBERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      GANBERT_cls = GANBERT(**hyper_param_Pre_training_stage)



      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": GANBERT_dic['sensitive_term'],
      "epochs_adv":GANBERT_dic['epochs_adv'],
      "iterations":GANBERT_dic['iterations'],
      "noise_size":GANBERT_dic['noise_size'],
      "l":GANBERT_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = GANBERT_cls.pre_training(**hyper_param_pre_training)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # Fair_GANBERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_name = "FairGANBERT"


      path_fairNda =  experiment_path + "/FairGANBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairGANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairGANBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      hyper_param_Training_eval = {
      "adversary":adv_fair_GANBERT_GF,
      "supervised_loss":Arch.supervised_loss,
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_GANBERT_dic['seed'],
      "discriminator_LR":Fair_GANBERT_dic['discriminator_LR'],
      "generator_LR":Fair_GANBERT_dic['generator_LR'],
      "adversary_LR":Fair_GANBERT_dic['adversary_LR'],
      "lda":Fair_GANBERT_dic['lda'],
      "batch_size": Fair_GANBERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      "flag_transformer": True
      }

      Adv_debiasing = FairGANBERT(**hyper_param_Training_eval)

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_GANBERT_dic['sensitive_term'],
      "iterations":Fair_GANBERT_dic['iterations'],
      "epochs":Fair_GANBERT_dic['epochs'],
      "num_mini_batch":Fair_GANBERT_dic['num_mini_batch'],
      "noise_size":Fair_GANBERT_dic['noise_size'],
      "selection_score":Fair_GANBERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)



      # return [performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine], [performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair],experiment_path






   def Model_initializer_BERT_NET(self, data_dic, BERT_dic, Fair_BERT_dic,dataset_name = "Noname", Exp = "Exp0"):

      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/" +dataset_name+"/"+ Exp+"/"+str(data_dic['label_ratio'])+"/"
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )


      self.path_maker(experiment_path + "/BERT/"+ "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "/BERT/"+ "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_FB = Arch.BertClassifier(2)
      model_FB = model_FB.to(device)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

      adv_FB = Arch.Adversary(data_dic['seed'],2)
      adv_FB = adv_FB.to(device)



      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "/BERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "BERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      #(self, classifier, adversary,supervised_loss, loss_adversary, log_training,plot_training,seed,classifier_LR=2e-5, adversary_LR=0.001, lda=[15, 30],batch_size = 32, path="", model_name="", transformer_name=""):
      
      

      hyper_param_Pre_training_stage = {
      "classifier":model_FB,
      "adversary":adv_FB,
      "supervised_loss":torch.nn.CrossEntropyLoss(),
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":BERT_dic['seed'],
      "classifier_LR":BERT_dic['classifier_LR'],
      "adversary_LR":BERT_dic['adversary_LR'],
      "lda":BERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      BERT_cls = BERT(**hyper_param_Pre_training_stage)


      #pre_training(self,train_loader,validation_loader,mode= "Validation", epochs_adv = 1, iterations = 1,noise_size = 100, l = 0.85,ippts_loader= None,fine_terms_list = None,path ="", model_name = "",transformer_name = ""):
      
      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = data_dic['label_ratio'])
      train_data_loader,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      print("eeeeeeeeeeeeeeex1", next(iter(train_data_loader)))

      hyper_param_pre_training = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": BERT_dic['sensitive_term'],
      "epochs_adv":BERT_dic['epochs_adv'],
      "iterations":BERT_dic['iterations'],
      "noise_size":BERT_dic['noise_size'],
      "l":BERT_dic['Mixup_lambda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = BERT_cls.pre_training(**hyper_param_pre_training)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # Fair_BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_name = "FairBERT"


      path_fairNda =  experiment_path + "/FairBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      # (self, adversary,loss_classifier,loss_adversary,optimizer_adv_lr = 0.001,optimizer_cl_lr =1e-5,lda= [1,1],batch_size = 8,path ="", model_name = "",transformer_name = ""):

      hyper_param_Training_eval = {
      "adversary":adv_FB,
      "loss_classifier":torch.nn.CrossEntropyLoss(),
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_BERT_dic['seed'],
      "classifier_LR":BERT_dic['classifier_LR'],
      "adversary_LR":BERT_dic['adversary_LR'],
      "lda":Fair_BERT_dic['lda'],
      "batch_size": Fair_BERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      }

      Adv_debiasing = FairBERT(**hyper_param_Training_eval)

      # train_eval(self, train_loader ,validation_loader,train_set,fairness_metric = "equ_odds_percent",max_shift_acc = 0.07,ippts_loader = None,fine_terms_list = None, iterations = 15,epochs = 1, num_mini_batch =1,noise_size =100, selection_score = "eq_odds",path ="", model_name = "",transformer_name = ""):

      hyper_param_train_eval = {
      "train_loader":train_data_loader,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_BERT_dic['sensitive_term'],
      "iterations":Fair_BERT_dic['iterations'],
      "epochs":Fair_BERT_dic['epochs'],
      "num_mini_batch":Fair_BERT_dic['num_mini_batch'],
      "noise_size":Fair_BERT_dic['noise_size'],
      "selection_score":Fair_BERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)



      # return [performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine], [performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair],experiment_path




   def Model_initializer_Full_supervised_BERT_NET(self, data_dic, BERT_dic, Fair_BERT_dic,dataset_name = "Noname", Exp = "Exp0"):

      out_dropout_rate = 0.3
      NUM_CLS_G = 3

      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # dataset loader
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------


      experiment_path = "/content/drive/MyDrive/SS_Fair/Experiments/" +dataset_name+"/"+ Exp+"/"+str(data_dic['label_ratio'])+"/"
      self.path_maker(experiment_path)


      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = 1)
      train_data_loader1,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )
      print("train***************************************",train)
      print("1"*30, next(iter(train_data_loader1)))

      self.path_maker(experiment_path + "/FullBERT/"+ "seed_"+str(data_dic['seed']))

      path_data = experiment_path + "/FullBERT/"+ "seed_"+str(data_dic['seed'])+"/"+ "data/"
      self.path_maker(path_data)

      Plot_data(train ,path_data).plot_generator(seed = data_dic['seed'], dataset = "train_"+dataset_name)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_FB = Arch.BertClassifier(2)
      model_FB = model_FB.to(device)
      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

      adv_FB = Arch.Adversary(data_dic['seed'],2)
      adv_FB = adv_FB.to(device)



      #-----------------------------------------------------------------------------------------------------------------
      # model initializer
      #------------------------------------------------------------------------------------------------------------------

      path = experiment_path + "/FullBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path)

      model_name = "FullBERT"

      log_training = TrainingLog(model_name,  data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training = PlottingUtils(path,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      path_nda = path
      model_nda = "{}_dis_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_nda = "{}_bert_BERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      #(self, classifier, adversary,supervised_loss, loss_adversary, log_training,plot_training,seed,classifier_LR=2e-5, adversary_LR=0.001, lda=[15, 30],batch_size = 32, path="", model_name="", transformer_name=""):
      data_dic['dataset_initializer'].sampler_object_experiment1(seed = data_dic['seed'], percentage = 1)
      train_data_loader1,debias_dataset,test_data_loader,train,test =  data_dic['dataset_initializer'].pop_data_loaders(batch_size = data_dic['batch_size'],MAX_LEN = data_dic['max_len'],
                                                                                                    GANBERT_flag = data_dic['GANBERT_flag'], IW_flag = data_dic['IW_flag'] )

      hyper_param_Pre_training_stage = {
      "classifier":model_FB,
      "adversary":adv_FB,
      "supervised_loss":torch.nn.CrossEntropyLoss(),
      "loss_adversary": Arch.loss_criterion,
      "log_training": log_training,
      "plot_training": plot_training,
      "seed":BERT_dic['seed'],
      "classifier_LR":BERT_dic['classifier_LR'],
      "adversary_LR":BERT_dic['adversary_LR'],
      "lda":BERT_dic['lda'],
      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      BERT_cls = BERT(**hyper_param_Pre_training_stage)


      #pre_training(self,train_loader,validation_loader,mode= "Validation", epochs_adv = 1, iterations = 1,noise_size = 100, l = 0.85,ippts_loader= None,fine_terms_list = None,path ="", model_name = "",transformer_name = ""):
      print("5"*30, next(iter(train_data_loader1)))

      hyper_param_pre_training = {
      "train_loader":train_data_loader1,
      "validation_loader":test_data_loader,
      "ippts_loader":test_data_loader,
      "mode":"Validation",

      "fine_terms_list": BERT_dic['sensitive_term'],
      "epochs_adv":BERT_dic['epochs_adv'],
      "iterations":BERT_dic['iterations'],
      "noise_size":BERT_dic['noise_size'],
      "l":BERT_dic['Mixup_lambda'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda
      }

      performance_log, performance_log_fine_terms,epoch_coarse,epoch_fine = BERT_cls.pre_training(**hyper_param_pre_training)


      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------
      # Fair_BERT
      #-----------------------------------------------------------------------------------------------------------------
      #-----------------------------------------------------------------------------------------------------------------



      model_name = "FairFullBERT"


      path_fairNda =  experiment_path + "/FairFullBERT/"+ "seed_"+str(data_dic['seed'])+"/"
      self.path_maker(path_fairNda)

      model_fairNda = "{}_dis_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])
      tr_name_fairNda = "{}_bert_FairBERT_{}.pt".format(data_dic['dataset_initializer'].get_num_label(),data_dic['seed'])

      log_training_fair = TrainingLog(model_name, data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())
      plot_training_fair = PlottingUtils(path_fairNda,model_name,data_dic['seed'], data_dic['dataset_initializer'].get_num_label(),data_dic['dataset_initializer'].get_num_unlabel())

      # (self, adversary,loss_classifier,loss_adversary,optimizer_adv_lr = 0.001,optimizer_cl_lr =1e-5,lda= [1,1],batch_size = 8,path ="", model_name = "",transformer_name = ""):

      hyper_param_Training_eval = {
      "adversary":adv_FB,
      "loss_classifier":torch.nn.CrossEntropyLoss(),
      "loss_adversary":Arch.loss_criterion,
      "log_training":log_training_fair,
      "plot_training": plot_training_fair,

      "seed":Fair_BERT_dic['seed'],
      "classifier_LR":BERT_dic['classifier_LR'],
      "adversary_LR":BERT_dic['adversary_LR'],
      "lda":Fair_BERT_dic['lda'],
      "batch_size": Fair_BERT_dic['batch_size'],

      "path" :  path_nda,
      "model_name" : model_nda,
      "transformer_name" :tr_name_nda,
      }

      Adv_debiasing = FairBERT(**hyper_param_Training_eval)

      # train_eval(self, train_loader ,validation_loader,train_set,fairness_metric = "equ_odds_percent",max_shift_acc = 0.07,ippts_loader = None,fine_terms_list = None, iterations = 15,epochs = 1, num_mini_batch =1,noise_size =100, selection_score = "eq_odds",path ="", model_name = "",transformer_name = ""):

      hyper_param_train_eval = {
      "train_loader":train_data_loader1,
      "validation_loader":test_data_loader,
      "train_set":debias_dataset,
      "ippts_loader":test_data_loader,

      "fine_terms_list":Fair_BERT_dic['sensitive_term'],
      "iterations":Fair_BERT_dic['iterations'],
      "epochs":Fair_BERT_dic['epochs'],
      "num_mini_batch":Fair_BERT_dic['num_mini_batch'],
      "noise_size":Fair_BERT_dic['noise_size'],
      "selection_score":Fair_BERT_dic['selection_score'],

      "path" :  path_fairNda,
      "model_name" : model_fairNda,
      "transformer_name" :tr_name_fairNda
      }

      performance_log_fair, performance_log_fine_terms_fair,epoch_coarse_fair,epoch_fine_fair= Adv_debiasing.train_eval(**hyper_param_train_eval)



























'''
you must initialized TrainingLog object here and send it to FairTextClassification and then save the log for each epocha and seed and model
'''
'''
you have one seed for data to set and also for model initalization that you must consider
'''

'''
you must initialized TrainingLog object here and send it to FairTextClassification and then save the log for each epocha and seed and model
'''
'''
you have one seed for data to set and also for model initalization that you must consider
'''
class FairTextClassification:

    def __init__(self):

        # self.classifier = classifier
        # self.debiasing_method = debiasing_method

        # self.classifier_dic = classifier_dic
        # self.debiasing_dic = debiasing_dic
        # self.data_dic = data_dic

        # self.seed_list = seed_list
        pass



    def Performance_Experience1(self,Model_initializer_list,data_dic, classifier_dic, debiasing_dic, seed_list, label_ratio,dataset_name = "No_name"):




      for i in range(len( classifier_dic)):
        for seed in seed_list:

          print("Seed",seed)

          '''
          - model_initializer class should recieve all the dictuionaries and we changfe the paraeter based on the experience, for example here seeds changes,
            classifier and debiasing_method is the output of model_initializer method

          - then find a way to save them properly.
          - then you have to make plot out of csv saved files and save them.
          - put the codes for all the models in folders.
          - define two experiments.

          '''
          data_dic[i]['label_ratio'] = label_ratio

          print("classifier_dic",classifier_dic)
          print(" Model_initializer_list[i]", Model_initializer_list[i])
          data_dic[i]['seed'] =  seed
          classifier_dic[i]['seed'] =  seed
          debiasing_dic[i]['seed'] =  seed


          Model_initializer_list[i]( data_dic[i], classifier_dic[i], debiasing_dic[i],dataset_name,"Exp1")







race_term = set(['asian','black','latino','white','other_race_or_ethnicity'])
gender_term = set(['bisexual','female','heterosexual', 'homosexual_gay_or_lesbian','male',"transgender",'other_gender', 'other_sexual_orientation'])
sensitive_term = [gender_term,race_term]
#seed_list= [11,12,13,14,15]
seed_list= [11]

iter_cls = 10
iter_debias = 25
unlabel_number = 5000

# # 10768 = [0.0025: 32, 0.005:54 ,0.01: 108,0.05: 538, 0.1: 1076, 0.3: 3230, 0.5: 5384], toxic_label = [0.1,0.45,0.45], non_toxic_label = [0.8,0.1,0.1]
# add 0.005, 0.0025 of samples

label_ratio = [0.005,0.01, 0.05, 0.1]


data_dic = {
    'dataset_initializer':jigsaw_initializer(),
    'seed': None,
    'label_ratio': None,
    'batch_size':32,
    'max_len': 200,
    'GANBERT_flag': True,
    'IW_flag':False
    }



data_dic_BERT = {
    'dataset_initializer':jigsaw_initializer(),
    'seed': None,
    'label_ratio': None,
    'batch_size':32,
    'max_len': 200,
    'GANBERT_flag': False,
    'IW_flag':False
    }

data_dict_list = [data_dic,data_dic,data_dic_BERT,data_dic_BERT]



NDA_dic ={
"seed": None,
"discriminator_LR":5e-5,
"generator_LR":3e-4,
"adversary_LR":0.0001,
"lda":[1,1],
"sensitive_term":sensitive_term,
"epochs_adv":1,
"iterations":iter_cls,
"noise_size":100,
"Mixup_lambda":0.85,

}

GANBERT_dic ={
"seed": None,
"discriminator_LR":5e-5,
"generator_LR":3e-4,
"adversary_LR":0.0001,
"lda":[1,1],
"sensitive_term":sensitive_term,
"epochs_adv":1,
"iterations":iter_cls,
"noise_size":100,
"Mixup_lambda":0.85,
}

BERT_dic={
"seed":None,
"classifier_LR":2e-5,
"adversary_LR":0.0001,
"lda":[1,1],
"sensitive_term": sensitive_term,
"epochs_adv":1,
"iterations":iter_cls,
"noise_size":100,
"Mixup_lambda":None,
}


classifier_dic = [NDA_dic,GANBERT_dic, BERT_dic,BERT_dic]


Fair_NDABERT = {
"seed": None,
"discriminator_LR":5e-5,
"generator_LR":3e-4,
"adversary_LR":0.0001,
"lda":[10,10],
"batch_size":32,
"sensitive_term":sensitive_term,
"iterations":iter_debias,
"epochs":1,
"num_mini_batch":1,
"noise_size": 100,
"selection_score":"eq_odds"
}


Fair_GANBERT = {
"seed": None,
"discriminator_LR":5e-5,
"generator_LR":3e-4,
"adversary_LR":0.0001,
"lda":[10,10],
"batch_size":32,
"sensitive_term":sensitive_term,
"iterations":iter_debias,
"epochs":1,
"num_mini_batch":1,
"noise_size": 100,
"selection_score":"eq_odds"
}



Fair_BERT_dic= {
"seed":None,
"classifier_LR":2e-5,
"adversary_LR":0.0001,
"lda":[60,60],
"batch_size":32,
"sensitive_term":sensitive_term,
"iterations":iter_debias,
"Mixup_lambda":None,
"epochs":1,
"num_mini_batch":1,
"noise_size": 100,
"selection_score":"eq_odds"
}


Fair_FullBERT_dic= {
"seed":None,
"classifier_LR":2e-5,
"adversary_LR":0.0001,
"lda":[60,60],
"batch_size":32,
"sensitive_term":sensitive_term,
"iterations":iter_debias,
"Mixup_lambda":None,
"epochs":1,
"num_mini_batch":1,
"noise_size": 100,
"selection_score":"eq_odds"
}



for i in range(len(label_ratio)):

  debiasing_dic = [Fair_NDABERT,Fair_GANBERT,Fair_BERT_dic,Fair_FullBERT_dic]
  

  Model_initializer_list = [model_initializer().Model_initializer_NDABERT_two_NET,model_initializer().Model_initializer_GANBERT_two_NET,model_initializer().Model_initializer_BERT_NET,model_initializer().Model_initializer_Full_supervised_BERT_NET]
  
 



  output = FairTextClassification().Performance_Experience1(Model_initializer_list,data_dict_list, classifier_dic, debiasing_dic, seed_list,label_ratio[i], "jigsaw")
  #root_path = "/content/drive/MyDrive/SS_Fair/Experiments/"
  #remove_files_with_pt_format(root_path)